# -*- coding: utf-8 -*-
"""Copy_of_Inference_with_Mask2Former.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WSjl4S2htVj3td3lwM2sHB9xE1vWQLOa
"""

#!pip install -q git+https://github.com/huggingface/transformers.git

"""## Define model

Next, we instantiate a Mask2Former model from the [hub](https://huggingface.co/models?other=mask2former), along with its image processor.

Note that the authors open-sourced many checkpoints, trained on instance, semantic and panoptic segmentation datasets. Here we load a Mask2Former checkpoint trained on the COCO-panoptic dataset.
"""

from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation
from PIL import Image
import requests
import numpy as np
from collections import defaultdict
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib import cm
processor = AutoImageProcessor.from_pretrained("facebook/mask2former-swin-base-coco-panoptic")
model = Mask2FormerForUniversalSegmentation.from_pretrained("facebook/mask2former-swin-base-coco-panoptic")
def get_mask(segment_id):
  print("Visualizing mask for:", model.config.id2label[segment_to_label[segment_id]])

  mask = (results['segmentation'].numpy() == segment_id)
  visual_mask = (mask * 255).astype(np.uint8)
  visual_mask = Image.fromarray(visual_mask)

  return visual_mask
def draw_panoptic_segmentation(segmentation, segments_info):
    # get the used color map
    viridis = cm.get_cmap('viridis', torch.max(segmentation))
    fig, ax = plt.subplots()
    ax.imshow(segmentation)
    instances_counter = defaultdict(int)
    handles = []
    # for each segment, draw its legend
    for segment in segments_info:
        segment_id = segment['id']
        segment_label_id = segment['label_id']
        segment_label = model.config.id2label[segment_label_id]
        label = f"{segment_label}-{instances_counter[segment_label_id]}"
        instances_counter[segment_label_id] += 1
        color = viridis(segment_id)
        handles.append(mpatches.Patch(color=color, label=label))

    ax.legend(handles=handles)

"""## Load image

I usually run inference on the cats image. It's part of the COCO dataset üêà.
"""

url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)
image

"""We prepare the image for the model using the image processor."""

inputs = processor(images=image, return_tensors="pt")
for k,v in inputs.items():
  print(k,v.shape)
import torch

with torch.no_grad():
  outputs = model(**inputs)

  # you can pass them to processor for postprocessing
results = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]
print(results.keys())
for segment in results['segments_info']:
  print(segment)

segment_to_label = {segment['id']: segment['label_id'] for segment in results["segments_info"]}
print(segment_to_label)

url = 'https://cdn-media.huggingface.co/Inference-API/Sample-results-on-the-Cityscapes-dataset-The-above-images-show-how-our-method-can-handle.png'
file = '/content/testimage1jpg'
image  =Image.open(file)
image
#image = Image.open(requests.get(url, stream=True).raw)
#image
inputs = processor(images=image, return_tensors="pt")
for k,v in inputs.items():
  print(k,v.shape)
import torch

with torch.no_grad():
  outputs = model(**inputs)

  # you can pass them to processor for postprocessing
results = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]
print(results.keys())
for segment in results['segments_info']:
  print(segment)
draw_panoptic_segmentation(**results)

url = 'https://cdn-media.huggingface.co/Inference-API/Sample-results-on-the-Cityscapes-dataset-The-above-images-show-how-our-method-can-handle.png'
file = '/content/testcase2.jpg'
image  =Image.open(file)
image
#image = Image.open(requests.get(url, stream=True).raw)
#image
inputs = processor(images=image, return_tensors="pt")
for k,v in inputs.items():
  print(k,v.shape)
import torch

with torch.no_grad():
  outputs = model(**inputs)

  # you can pass them to processor for postprocessing
results = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]
print(results.keys())
for segment in results['segments_info']:
  print(segment)
draw_panoptic_segmentation(**results)

url = 'https://cdn-media.huggingface.co/Inference-API/Sample-results-on-the-Cityscapes-dataset-The-above-images-show-how-our-method-can-handle.png'
file = '/content/testcase3.jpg'
image  =Image.open(file)
image
#image = Image.open(requests.get(url, stream=True).raw)
#image
inputs = processor(images=image, return_tensors="pt")
for k,v in inputs.items():
  print(k,v.shape)
import torch

with torch.no_grad():
  outputs = model(**inputs)

  # you can pass them to processor for postprocessing
results = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]
print(results.keys())
for segment in results['segments_info']:
  print(segment)
draw_panoptic_segmentation(**results)

url = 'https://cdn-media.huggingface.co/Inference-API/Sample-results-on-the-Cityscapes-dataset-The-above-images-show-how-our-method-can-handle.png'
file = '/content/testcase4.jpg'
image  =Image.open(file)
image
#image = Image.open(requests.get(url, stream=True).raw)
#image
inputs = processor(images=image, return_tensors="pt")
for k,v in inputs.items():
  print(k,v.shape)
import torch

with torch.no_grad():
  outputs = model(**inputs)

  # you can pass them to processor for postprocessing
results = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]
print(results.keys())
for segment in results['segments_info']:
  print(segment)
draw_panoptic_segmentation(**results)

url = 'https://cdn-media.huggingface.co/Inference-API/Sample-results-on-the-Cityscapes-dataset-The-above-images-show-how-our-method-can-handle.png'
file = '/content/testcase5.jpg'
image  =Image.open(file)
image
#image = Image.open(requests.get(url, stream=True).raw)
#image
inputs = processor(images=image, return_tensors="pt")
for k,v in inputs.items():
  print(k,v.shape)
import torch

with torch.no_grad():
  outputs = model(**inputs)

  # you can pass them to processor for postprocessing
results = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]
print(results.keys())
for segment in results['segments_info']:
  print(segment)
draw_panoptic_segmentation(**results)

url = 'https://cdn-media.huggingface.co/Inference-API/Sample-results-on-the-Cityscapes-dataset-The-above-images-show-how-our-method-can-handle.png'
file = '/content/testcase6.jpg'
image  =Image.open(file)
image
#image = Image.open(requests.get(url, stream=True).raw)
#image
inputs = processor(images=image, return_tensors="pt")
for k,v in inputs.items():
  print(k,v.shape)
import torch

with torch.no_grad():
  outputs = model(**inputs)

  # you can pass them to processor for postprocessing
results = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]
print(results.keys())
for segment in results['segments_info']:
  print(segment)
draw_panoptic_segmentation(**results)

url = 'https://cdn-media.huggingface.co/Inference-API/Sample-results-on-the-Cityscapes-dataset-The-above-images-show-how-our-method-can-handle.png'
file = '/content/testcase7.jpg'
image  =Image.open(file)
image
#image = Image.open(requests.get(url, stream=True).raw)
#image
inputs = processor(images=image, return_tensors="pt")
for k,v in inputs.items():
  print(k,v.shape)
import torch

with torch.no_grad():
  outputs = model(**inputs)

  # you can pass them to processor for postprocessing
results = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]
print(results.keys())
for segment in results['segments_info']:
  print(segment)
draw_panoptic_segmentation(**results)

url = 'https://cdn-media.huggingface.co/Inference-API/Sample-results-on-the-Cityscapes-dataset-The-above-images-show-how-our-method-can-handle.png'
file = '/content/testcase8.jpg'
image  =Image.open(file)
image
#image = Image.open(requests.get(url, stream=True).raw)
#image
inputs = processor(images=image, return_tensors="pt")
for k,v in inputs.items():
  print(k,v.shape)
import torch

with torch.no_grad():
  outputs = model(**inputs)

  # you can pass them to processor for postprocessing
results = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]
print(results.keys())
for segment in results['segments_info']:
  print(segment)
draw_panoptic_segmentation(**results)

url = 'https://cdn-media.huggingface.co/Inference-API/Sample-results-on-the-Cityscapes-dataset-The-above-images-show-how-our-method-can-handle.png'
file = '/content/testcase9.jpg'
image  =Image.open(file)
image
#image = Image.open(requests.get(url, stream=True).raw)
#image
inputs = processor(images=image, return_tensors="pt")
for k,v in inputs.items():
  print(k,v.shape)
import torch

with torch.no_grad():
  outputs = model(**inputs)

  # you can pass them to processor for postprocessing
results = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]
print(results.keys())
for segment in results['segments_info']:
  print(segment)
draw_panoptic_segmentation(**results)

url = 'https://cdn-media.huggingface.co/Inference-API/Sample-results-on-the-Cityscapes-dataset-The-above-images-show-how-our-method-can-handle.png'
file = '/content/testcase10.jpg'
image  =Image.open(file)
image
#image = Image.open(requests.get(url, stream=True).raw)
#image
inputs = processor(images=image, return_tensors="pt")
for k,v in inputs.items():
  print(k,v.shape)
import torch

with torch.no_grad():
  outputs = model(**inputs)

  # you can pass them to processor for postprocessing
results = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]
print(results.keys())
for segment in results['segments_info']:
  print(segment)
draw_panoptic_segmentation(**results)

url = 'https://cdn-media.huggingface.co/Inference-API/Sample-results-on-the-Cityscapes-dataset-The-above-images-show-how-our-method-can-handle.png'
file = '/content/testcase10.jpg'
image  =Image.open(file)
image
#image = Image.open(requests.get(url, stream=True).raw)
#image
inputs = processor(images=image, return_tensors="pt")
for k,v in inputs.items():
  print(k,v.shape)
import torch

with torch.no_grad():
  outputs = model(**inputs)

  # you can pass them to processor for postprocessing
results = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]
print(results.keys())
for segment in results['segments_info']:
  print(segment)
draw_panoptic_segmentation(**results)

"""## Forward pass

Next, we can forward the `pixel_values` and `pixel_mask` through the model. We use the `torch.no_grad()` context manager as we don't need gradients to be computed (this is only useful when training a model). This will save us a lot of memory.
"""

# note: segment with id == 0 means "background",
# so we visualize segment with id == 1 here
get_mask(segment_id=1)

"""## Visualize

For visualization, we can postprocess the outputs of the model. In case of panoptic segmentation, the model predicts 2 things per image: 1) a segmentation map 2) a corresponding segments_info (which contains more information regarding the segments in the segmentation map).
"""



segment_to_label = {segment['id']: segment['label_id'] for segment in results["segments_info"]}
print(segment_to_label)

"""In this case, the model has recognized 5 different segments in the image."""

import numpy as np

def get_mask(segment_id):
  print("Visualizing mask for:", model.config.id2label[segment_to_label[segment_id]])

  mask = (results['segmentation'].numpy() == segment_id)
  visual_mask = (mask * 255).astype(np.uint8)
  visual_mask = Image.fromarray(visual_mask)

  return visual_mask

# note: segment with id == 0 means "background",
# so we visualize segment with id == 1 here
get_mask(segment_id=2)

"""Let's visualize the binary mask of the first segment:"""



"""We can create a single visualization for all segments:"""



"""Here, we can see that the model is capable of detecting the individual cats and remotes in the image. Semantic segmentation on the other hand would just create a single mask for the "cat" category.

## Inference (semantic segmentation)

We can do the same as above, but with a checkpoint fine-tuned on a semantic segmentation dataset (in this case, CityScapes).
"""

from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation

processor = AutoImageProcessor.from_pretrained("facebook/mask2former-swin-small-cityscapes-semantic")
model = Mask2FormerForUniversalSegmentation.from_pretrained("facebook/mask2former-swin-small-cityscapes-semantic")

"""Let's load an image from the CityScapes dataset."""

from PIL import Image
import requests

url = 'https://cdn-media.huggingface.co/Inference-API/Sample-results-on-the-Cityscapes-dataset-The-above-images-show-how-our-method-can-handle.png'
image = Image.open(requests.get(url, stream=True).raw)
image

inputs = processor(images=image, return_tensors="pt")
for k,v in inputs.items():
  print(k,v.shape)

import torch

# forward pass
with torch.no_grad():
  outputs = model(**inputs)

# you can pass them to processor for postprocessing
predicted_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]
print(predicted_map.shape)

"""We can draw the semantic segmentation map (with one label per pixel) over the image:"""

# generate random color palette, which maps each class to a RGB value
color_palette = [list(np.random.choice(range(256), size=3)) for _ in range(len(model.config.id2label))]
print(color_palette)

from torch import nn
import numpy as np
import matplotlib.pyplot as plt

seg = predicted_map
color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8) # height, width, 3
palette = np.array(color_palette)
for label, color in enumerate(palette):
    color_seg[seg == label, :] = color
# Convert to BGR
color_seg = color_seg[..., ::-1]

# Show image + mask
img = np.array(image) * 0.5 + color_seg * 0.5
img = img.astype(np.uint8)

plt.figure(figsize=(15, 10))
plt.imshow(img)
plt.show()

"""## Inference (instance segmentation)

Inference for instance segmentation is very similar.
"""

from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation

processor = AutoImageProcessor.from_pretrained("facebook/mask2former-swin-large-coco-instance")
model = Mask2FormerForUniversalSegmentation.from_pretrained("facebook/mask2former-swin-large-coco-instance")

from PIL import Image
import requests

url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)
image

inputs = processor(images=image, return_tensors="pt")
for k,v in inputs.items():
  print(k,v.shape)

import torch

# forward pass
with torch.no_grad():
  outputs = model(**inputs)

outputs.keys()

torch.where(outputs.class_queries_logits.argmax(-1) == 65)

outputs.masks_queries_logits[:,160,:,:]

# you can pass them to processor for postprocessing
results = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]], threshold=0.9)[0]
print(results.keys())

results['segmentation']

for segment in results['segments_info']:
  print(segment)

segment_to_label = {segment['id']: segment['label_id'] for segment in results["segments_info"]}
print(segment_to_label)

import numpy as np

def get_mask(segment_id):
  print("Visualizing mask for:", model.config.id2label[segment_to_label[segment_id]])

  mask = (results['segmentation'].numpy() == segment_id)
  visual_mask = (mask * 255).astype(np.uint8)
  visual_mask = Image.fromarray(visual_mask)

  return visual_mask

# note: segment with id == 0 means "background",
# so we visualize segment with id == 1 here
get_mask(segment_id=1)

get_mask(segment_id=2)

get_mask(segment_id=3)

get_mask(segment_id=4)

